{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/'\n",
    "train_file = 'train.csv'\n",
    "train_file2 = 'train_2.csv'  # former train data\n",
    "test_file = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.read_csv(os.path.join(data_root, train_file))\n",
    "trainset.append(pd.read_csv(os.path.join(data_root, train_file2)))\n",
    "testset = pd.read_csv(os.path.join(data_root, test_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset['n_jobs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>penalty</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>alpha</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>random_state</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_clusters_per_class</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>flip_y</th>\n",
       "      <th>scale</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>0.304083</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>417</td>\n",
       "      <td>475</td>\n",
       "      <td>-1</td>\n",
       "      <td>1089</td>\n",
       "      <td>327</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>24.242009</td>\n",
       "      <td>0.409987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.727744</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>578</td>\n",
       "      <td>569</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>373</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>54.626302</td>\n",
       "      <td>3.950953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>0.745885</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>588</td>\n",
       "      <td>529</td>\n",
       "      <td>2</td>\n",
       "      <td>428</td>\n",
       "      <td>1198</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>17.999964</td>\n",
       "      <td>0.368702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>none</td>\n",
       "      <td>0.474605</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>829</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>877</td>\n",
       "      <td>313</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>82.257222</td>\n",
       "      <td>1.004559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.395049</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>167</td>\n",
       "      <td>418</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>644</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>95.515601</td>\n",
       "      <td>0.802800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     penalty  l1_ratio   alpha  max_iter  random_state  n_jobs  \\\n",
       "0   0        none  0.304083  0.0001       417           475      -1   \n",
       "1   1          l1  0.727744  0.0010       578           569       1   \n",
       "2   2        none  0.745885  0.0100       588           529       2   \n",
       "3   3        none  0.474605  0.0010       829           103       4   \n",
       "4   4  elasticnet  0.395049  0.0010       167           418       2   \n",
       "\n",
       "   n_samples  n_features  n_classes  n_clusters_per_class  n_informative  \\\n",
       "0       1089         327          4                     3              7   \n",
       "1        790         373          4                     5              7   \n",
       "2        428        1198          2                     5              6   \n",
       "3        877         313          6                     5              7   \n",
       "4        216         644          8                     5             11   \n",
       "\n",
       "     flip_y      scale      time  \n",
       "0  0.074798  24.242009  0.409987  \n",
       "1  0.077781  54.626302  3.950953  \n",
       "2  0.030196  17.999964  0.368702  \n",
       "3  0.057261  82.257222  1.004559  \n",
       "4  0.073728  95.515601  0.802800  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### general preprocessing\n",
    "drop features have very low correlations with time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(data, is_test=False):\n",
    "    data.set_index('id', inplace=True)\n",
    "    data.loc[data['n_jobs']==-1, 'n_jobs'] = data['n_jobs'].max()\n",
    "    data['1/n_jobs'] = 1 / data['n_jobs']\n",
    "    one_hot = pd.get_dummies(data['penalty'])\n",
    "    if not is_test:\n",
    "        time = data['time']\n",
    "        data = data.drop(['penalty','time'], axis=1).join(one_hot)\n",
    "    else:\n",
    "        time = None\n",
    "        data = data.drop(['penalty'], axis=1).join(one_hot)\n",
    "    data = data.drop(['l1_ratio','alpha','random_state','scale'], axis=1)\n",
    "    return data,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = data_preprocess(trainset)\n",
    "test_x, _ =  data_preprocess(testset, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_jobs                 -0.307182\n",
       "l2                     -0.223753\n",
       "none                   -0.222936\n",
       "n_clusters_per_class   -0.060997\n",
       "n_informative           0.109154\n",
       "flip_y                  0.120277\n",
       "n_classes               0.166696\n",
       "l1                      0.197349\n",
       "n_features              0.220124\n",
       "elasticnet              0.237999\n",
       "max_iter                0.264227\n",
       "n_samples               0.296451\n",
       "1/n_jobs                0.351546\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.corrwith(train_y).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_clusters_per_class</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>flip_y</th>\n",
       "      <th>1/n_jobs</th>\n",
       "      <th>elasticnet</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>602</td>\n",
       "      <td>4</td>\n",
       "      <td>1376</td>\n",
       "      <td>1078</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.049072</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>522</td>\n",
       "      <td>4</td>\n",
       "      <td>598</td>\n",
       "      <td>1528</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.041393</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>398</td>\n",
       "      <td>16</td>\n",
       "      <td>553</td>\n",
       "      <td>1008</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>439</td>\n",
       "      <td>16</td>\n",
       "      <td>1977</td>\n",
       "      <td>1319</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278</td>\n",
       "      <td>4</td>\n",
       "      <td>1826</td>\n",
       "      <td>1530</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0.050126</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_iter  n_jobs  n_samples  n_features  n_classes  n_clusters_per_class  \\\n",
       "id                                                                             \n",
       "0        602       4       1376        1078          3                     4   \n",
       "1        522       4        598        1528         10                     3   \n",
       "2        398      16        553        1008          8                     5   \n",
       "3        439      16       1977        1319          3                     4   \n",
       "4        278       4       1826        1530          7                     5   \n",
       "\n",
       "    n_informative    flip_y  1/n_jobs  elasticnet  l1  l2  none  \n",
       "id                                                               \n",
       "0               8  0.049072    0.2500           0   0   1     0  \n",
       "1              11  0.041393    0.2500           1   0   0     0  \n",
       "2               8  0.005987    0.0625           0   0   1     0  \n",
       "3              10  0.002964    0.0625           0   0   0     1  \n",
       "4               8  0.050126    0.2500           0   0   0     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(col):\n",
    "    \n",
    "    col = (col-col.min()) / (col.max()-col.min())\n",
    "    return col\n",
    "\n",
    "def new_features(data):\n",
    "    \n",
    "    data['f1'] = data['n_samples']*data['elasticnet']*data['max_iter']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f2'] = data['n_samples']*data['l1']*data['max_iter']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f3'] = data['n_samples']*data['none']*data['max_iter']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f4'] = data['n_samples']*data['l2']*data['max_iter']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    \n",
    "    data['f5'] = data['n_samples']*data['elasticnet']*data['max_iter']*data['flip_y']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f6'] = data['n_samples']*data['l1']*data['max_iter']*data['flip_y']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f7'] = data['n_samples']*data['none']*data['max_iter']*data['flip_y']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f8'] = data['n_samples']*data['l2']*data['max_iter']*data['flip_y']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    \n",
    "    data['f9'] = data['n_samples']*data['elasticnet']*data['max_iter']*data['flip_y']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f10'] = data['n_samples']*data['l1']*data['max_iter']*data['flip_y']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f11'] = data['n_samples']*data['none']*data['max_iter']*data['flip_y']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    data['f12'] = data['n_samples']*data['l2']*data['max_iter']*data['flip_y']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']\n",
    "    \n",
    "    data['f13'] = data['n_samples']*data['elasticnet']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['n_clusters_per_class']\n",
    "    data['f14'] = data['n_samples']*data['l1']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['n_clusters_per_class']\n",
    "    data['f15'] = data['n_samples']*data['none']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['n_clusters_per_class']\n",
    "    data['f16'] = data['n_samples']*data['l2']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['n_clusters_per_class']\n",
    "    \n",
    "    data['f17'] = data['n_samples']*data['elasticnet']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['flip_y']\n",
    "    data['f18'] = data['n_samples']*data['l1']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['flip_y']\n",
    "    data['f19'] = data['n_samples']*data['none']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['flip_y']\n",
    "    data['f20'] = data['n_samples']*data['l2']*data['max_iter']*data['n_informative']\\\n",
    "                *data['n_classes']*data['n_features']*data['1/n_jobs']*data['flip_y']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = new_features(train_x)\n",
    "test_x = new_features(test_x)\n",
    "\n",
    "# train_x['n_clusters'] = train_x['n_clusters_per_class'] * train_x['n_classes']\n",
    "# test_x['n_clusters'] = test_x['n_clusters_per_class'] * test_x['n_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_jobs                 -0.307182\n",
       "l2                     -0.223753\n",
       "none                   -0.222936\n",
       "f8                     -0.075477\n",
       "f20                    -0.069635\n",
       "f12                    -0.069635\n",
       "n_clusters_per_class   -0.060997\n",
       "f16                    -0.028879\n",
       "f4                     -0.009103\n",
       "f3                      0.026209\n",
       "f15                     0.046339\n",
       "f7                      0.051800\n",
       "f19                     0.059547\n",
       "f11                     0.059547\n",
       "n_informative           0.109154\n",
       "flip_y                  0.120277\n",
       "n_classes               0.166696\n",
       "l1                      0.197349\n",
       "n_features              0.220124\n",
       "elasticnet              0.237999\n",
       "max_iter                0.264227\n",
       "n_samples               0.296451\n",
       "1/n_jobs                0.351546\n",
       "f9                      0.550064\n",
       "f17                     0.550064\n",
       "f5                      0.550381\n",
       "f13                     0.567259\n",
       "f1                      0.622755\n",
       "f10                     0.645936\n",
       "f18                     0.645936\n",
       "f6                      0.651639\n",
       "f14                     0.668345\n",
       "f2                      0.685755\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.corrwith(train_y).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop unimportant features\n",
    "it seems do drop could have better result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_features(data, cols):\n",
    "    data.drop(cols, axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_cols = ['n_jobs', 'f12','n_clusters_per_class','f4','f3','f7','f11','f8','f16','f15','f19']\n",
    "# train_x = drop_features(train_x, drop_cols)\n",
    "# test_x = drop_features(test_x, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_clusters_per_class</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>flip_y</th>\n",
       "      <th>1/n_jobs</th>\n",
       "      <th>elasticnet</th>\n",
       "      <th>...</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417</td>\n",
       "      <td>8</td>\n",
       "      <td>1089</td>\n",
       "      <td>327</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.553559e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.887491e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.559197e+09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>373</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.709322e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.384470e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>588</td>\n",
       "      <td>2</td>\n",
       "      <td>428</td>\n",
       "      <td>1198</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.103970e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.462382e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.044804e+09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>4</td>\n",
       "      <td>877</td>\n",
       "      <td>313</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.954562e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.368194e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.194697e+10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>644</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.535975e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.110681e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_iter  n_jobs  n_samples  n_features  n_classes  n_clusters_per_class  \\\n",
       "id                                                                             \n",
       "0        417       8       1089         327          4                     3   \n",
       "1        578       1        790         373          4                     5   \n",
       "2        588       2        428        1198          2                     5   \n",
       "3        829       4        877         313          6                     5   \n",
       "4        167       2        216         644          8                     5   \n",
       "\n",
       "    n_informative    flip_y  1/n_jobs  elasticnet ...             f7   f8  \\\n",
       "id                                                ...                       \n",
       "0               7  0.074798     0.125           0 ...   5.553559e+06  0.0   \n",
       "1               7  0.077781     1.000           0 ...   0.000000e+00  0.0   \n",
       "2               6  0.030196     0.500           0 ...   9.103970e+06  0.0   \n",
       "3               7  0.057261     0.250           0 ...   1.954562e+07  0.0   \n",
       "4              11  0.073728     0.500           1 ...   0.000000e+00  0.0   \n",
       "\n",
       "              f9           f10           f11  f12           f13           f14  \\\n",
       "id                                                                              \n",
       "0   0.000000e+00  0.000000e+00  3.887491e+07  0.0  0.000000e+00  0.000000e+00   \n",
       "1   0.000000e+00  3.709322e+08  0.000000e+00  0.0  0.000000e+00  2.384470e+10   \n",
       "2   0.000000e+00  0.000000e+00  5.462382e+07  0.0  0.000000e+00  0.000000e+00   \n",
       "3   0.000000e+00  0.000000e+00  1.368194e+08  0.0  0.000000e+00  0.000000e+00   \n",
       "4   7.535975e+07  0.000000e+00  0.000000e+00  0.0  5.110681e+09  0.000000e+00   \n",
       "\n",
       "             f15  f16  \n",
       "id                     \n",
       "0   1.559197e+09  0.0  \n",
       "1   0.000000e+00  0.0  \n",
       "2   9.044804e+09  0.0  \n",
       "3   1.194697e+10  0.0  \n",
       "4   0.000000e+00  0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty_map = {\n",
    "#     'none': 0,\n",
    "#     'l1': 1,\n",
    "#     'l2': 2,\n",
    "#     'elasticnet': 3\n",
    "# }\n",
    "# # train_x['penalty'].value_counts()\n",
    "# trainset['penalty'] = trainset['penalty'].map(penalty_map)\n",
    "# testset['penalty'] = testset['penalty'].map(penalty_map)\n",
    "# trainset.set_index('id', inplace=True)\n",
    "# testset.set_index('id', inplace=True)\n",
    "# # trainset.head(10)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = trainset[trainset.columns.tolist()[:-1]]\n",
    "# train_y = trainset['time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here goes real feature engneering\n",
    "\n",
    "# n_jobs should be modified as level\n",
    "# baesed on raw n_jobs generate new features, sth divided by n_jobs\n",
    "# classes and clusters relations \n",
    "# Do not only focus on intra features, try to analysis characteristics of each feature itself\n",
    "# Normalize some features may be helpful\n",
    "# Use median to replace values in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_map = {\n",
    "#     1: 1,\n",
    "#     2: 2,\n",
    "#     4: 4,\n",
    "#     8: 8,\n",
    "#     16: 16,\n",
    "#     -1: 16\n",
    "# }\n",
    "\n",
    "# train_x['n_jobs'] = train_x['n_jobs'].map(jobs_map)\n",
    "# testset['n_jobs'] = testset['n_jobs'].map(jobs_map)\n",
    "\n",
    "# train_x['n_samples_per_job'] = train_x['n_samples'] / train_x['n_jobs']\n",
    "# testset['n_samples_per_job'] = testset['n_samples'] / testset['n_jobs']\n",
    "\n",
    "# jobs_map = {\n",
    "#     1: 1,\n",
    "#     2: 2,\n",
    "#     4: 3,\n",
    "#     8: 4,\n",
    "#     16: 5,\n",
    "#     -1: 5\n",
    "# }\n",
    "\n",
    "# train_x['lvl_jobs'] = train_x['n_jobs'].map(jobs_map)\n",
    "# testset['lvl_jobs'] = testset['n_jobs'].map(jobs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x['n_clusters'] = train_x['n_classes'] * train_x['n_clusters_per_class']\n",
    "# testset['n_clusters'] = testset['n_classes'] * testset['n_clusters_per_class']\n",
    "# # train_x['n_combine_info_feature'] = train_x['n_classes'] * train_x['n_clusters_per_class'] * train_x['n_informative']\n",
    "# # testset['n_combine_info_feature'] = testset['n_classes'] * testset['n_clusters_per_class'] * testset['n_informative']\n",
    "# train_x['informative_ratio'] = train_x['n_informative'] / train_x['n_features']\n",
    "# testset['informative_ratio'] = testset['n_informative'] / train_x['n_features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe it's not good, this is max_iter, not n_iter, can't simply multiple them\n",
    "# train_x['n_sample_total_run'] = train_x['n_samples'] * train_x['max_iter']\n",
    "# testset['n_sample_total_run'] = testset['n_samples'] * testset['max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x['scaled_features'] = train_x['scale'] * train_x['n_features']\n",
    "# testset['scaled_features'] = testset['scale'] * testset['n_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # penalty one hot\n",
    "\n",
    "# train_ohe = OneHotEncoder()\n",
    "# test_ohe = OneHotEncoder()\n",
    "\n",
    "# # one hot encode trainset\n",
    "# penalty_oh = train_ohe.fit_transform(train_x.penalty.values.reshape(-1,1)).toarray()\n",
    "# df_pnt_oh = pd.DataFrame(penalty_oh, columns=['pnt_none', 'pnt_l1', 'pnt_l2', 'pnt_elasticnet'])\n",
    "# train_x = pd.concat([train_x, df_pnt_oh], axis=1)\n",
    "\n",
    "# # onr hot encode testset\n",
    "# penalty_oh = test_ohe.fit_transform(testset.penalty.values.reshape(-1,1)).toarray()\n",
    "# df_pnt_oh = pd.DataFrame(penalty_oh, columns=['pnt_none', 'pnt_l1', 'pnt_l2', 'pnt_elasticnet'])\n",
    "# testset = pd.concat([testset, df_pnt_oh], axis=1)\n",
    "\n",
    "# del train_ohe\n",
    "# del test_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ohe = OneHotEncoder()\n",
    "# test_ohe = OneHotEncoder()\n",
    "\n",
    "# train_x['li_ratio_bin'] = Binarizer(threshold=0.5).fit_transform(train_x.l1_ratio.values.reshape(-1,1))\n",
    "# l1_ratio_oh = train_ohe.fit_transform(train_x.li_ratio_bin.values.reshape(-1,1)).toarray()\n",
    "# df_pnt_oh = pd.DataFrame(l1_ratio_oh, columns=['l1_ratio_l2', 'l1_ratio_l1',])\n",
    "# train_x = pd.concat([train_x, df_pnt_oh], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "\n",
    "# mmscaler_train = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "# mmscaler_test = MinMaxScaler(copy=True, feature_range=(0, 1))\n",
    "# mmscaler_train.fit(train_x.max_iter.values.reshape(-1,1))\n",
    "# train_x['max_iter'] = mmscaler_train.transform(train_x.max_iter.values.reshape(-1,1))\n",
    "# testset['max_iter'] = mmscaler_test.fit_transform(testset.max_iter.values.reshape(-1,1))\n",
    "# # normalize(train_x['n_classes'], axis=1, copy=False)\n",
    "# # normalize(testset['n_classes'], axis=1, copy=False)\n",
    "# # normalize(train_x['n_sample_total_run'], axis=1, copy=False)\n",
    "# # normalize(testset['n_sample_total_run'], axis=1, copy=False)\n",
    "# # normalize(train_x['n_samples_per_job'], axis=1, copy=False)\n",
    "# # normalize(testset['n_samples_per_job'], axis=1, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop useless columns here\n",
    "\n",
    "# train_x.drop(['penalty'], axis=1, inplace=True)\n",
    "# testset.drop(['penalty'], axis=1, inplace=True)\n",
    "# train_x.drop(['scale'], axis=1, inplace=True)\n",
    "# testset.drop(['scale'], axis=1, inplace=True)\n",
    "# train_x.drop(['alpha'], axis=1, inplace=True)\n",
    "# testset.drop(['alpha'], axis=1, inplace=True)\n",
    "# train_x.drop(['l1_ratio'], axis=1, inplace=True)\n",
    "# testset.drop(['l1_ratio'], axis=1, inplace=True)\n",
    "# train_x.drop(['n_clusters_per_class'], axis=1, inplace=True)\n",
    "# testset.drop(['n_clusters_per_class'], axis=1, inplace=True)\n",
    "# train_x.drop(['n_jobs'], axis=1 ,inplace=True)\n",
    "# testset.drop(['n_jobs'], axis=1, inplace=True)\n",
    "# train_x.drop(['random_state'], axis=1 ,inplace=True)\n",
    "# testset.drop(['random_state'], axis=1, inplace=True)\n",
    "# train_x.drop(['n_informative'], axis=1 ,inplace=True)\n",
    "# testset.drop(['n_informative'], axis=1, inplace=True)\n",
    "# train_x.drop(['n_features'], axis=1, inplace=True)\n",
    "# testset.drop(['n_features'], axis=1, inplace=True)\n",
    "# train_x.drop(['n_samples'], axis=1, inplace=True)\n",
    "# testset.drop(['n_samples'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([train_x, train_y], axis=1).corr()[['time']].sort_values(by='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonling/workspace/pyenv/scientific/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_iter</th>\n",
       "      <th>n_jobs</th>\n",
       "      <th>n_samples</th>\n",
       "      <th>n_features</th>\n",
       "      <th>n_classes</th>\n",
       "      <th>n_clusters_per_class</th>\n",
       "      <th>n_informative</th>\n",
       "      <th>flip_y</th>\n",
       "      <th>1/n_jobs</th>\n",
       "      <th>elasticnet</th>\n",
       "      <th>...</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417</td>\n",
       "      <td>8</td>\n",
       "      <td>1089</td>\n",
       "      <td>327</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.074798</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.887491e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.559197e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.887491e+07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "      <td>373</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.077781</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.384470e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.709322e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>588</td>\n",
       "      <td>2</td>\n",
       "      <td>428</td>\n",
       "      <td>1198</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.030196</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.462382e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.044804e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.462382e+07</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>829</td>\n",
       "      <td>4</td>\n",
       "      <td>877</td>\n",
       "      <td>313</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.057261</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.368194e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.194697e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.368194e+08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>167</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>644</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.073728</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.110681e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.535975e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_iter  n_jobs  n_samples  n_features  n_classes  n_clusters_per_class  \\\n",
       "id                                                                             \n",
       "0        417       8       1089         327          4                     3   \n",
       "1        578       1        790         373          4                     5   \n",
       "2        588       2        428        1198          2                     5   \n",
       "3        829       4        877         313          6                     5   \n",
       "4        167       2        216         644          8                     5   \n",
       "\n",
       "    n_informative    flip_y  1/n_jobs  elasticnet ...            f11  f12  \\\n",
       "id                                                ...                       \n",
       "0               7  0.074798     0.125           0 ...   3.887491e+07  0.0   \n",
       "1               7  0.077781     1.000           0 ...   0.000000e+00  0.0   \n",
       "2               6  0.030196     0.500           0 ...   5.462382e+07  0.0   \n",
       "3               7  0.057261     0.250           0 ...   1.368194e+08  0.0   \n",
       "4              11  0.073728     0.500           1 ...   0.000000e+00  0.0   \n",
       "\n",
       "             f13           f14           f15  f16           f17           f18  \\\n",
       "id                                                                              \n",
       "0   0.000000e+00  0.000000e+00  1.559197e+09  0.0  0.000000e+00  0.000000e+00   \n",
       "1   0.000000e+00  2.384470e+10  0.000000e+00  0.0  0.000000e+00  3.709322e+08   \n",
       "2   0.000000e+00  0.000000e+00  9.044804e+09  0.0  0.000000e+00  0.000000e+00   \n",
       "3   0.000000e+00  0.000000e+00  1.194697e+10  0.0  0.000000e+00  0.000000e+00   \n",
       "4   5.110681e+09  0.000000e+00  0.000000e+00  0.0  7.535975e+07  0.000000e+00   \n",
       "\n",
       "             f19  f20  \n",
       "id                     \n",
       "0   3.887491e+07  0.0  \n",
       "1   0.000000e+00  0.0  \n",
       "2   5.462382e+07  0.0  \n",
       "3   1.368194e+08  0.0  \n",
       "4   0.000000e+00  0.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(train_x, train_y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT: 2.531196966447541\n",
      "LASSO: 0.6393070318001428\n",
      "Ridge Regression:  0.6637223377459497\n",
      "XGBoost 0.689680729996766\n",
      "XGBoost 0.6983091286238777\n",
      "XGBoost 0.7039132813269746\n",
      "0.5645817414455705\n",
      "0.5941372636311134\n",
      "0.6789242576698143\n",
      "0.6473105733275659\n"
     ]
    }
   ],
   "source": [
    "# gbr = GradientBoostingRegressor(n_estimators=1500, learning_rate=0.1, max_depth=1, loss='ls')\n",
    "# gbr.fit(x_train, y_train)\n",
    "\n",
    "# # rfr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=1000)\n",
    "# # rfr.fit(x_train, y_train)\n",
    "\n",
    "# lsr= Lasso(alpha=0.01, normalize=False)\n",
    "# lsr.fit(x_train, y_train)\n",
    "\n",
    "# rgr= Ridge(alpha=0.01)\n",
    "# rgr.fit(x_train, y_train)\n",
    "\n",
    "# xgb = XGBRegressor(booster='gblinear', n_estimators=1100, learning_rate=0.1, max_depth=3)\n",
    "# xgb.fit(x_train, y_train)\n",
    "\n",
    "# xgb2 = XGBRegressor(booster='gblinear', n_estimators=1300, learning_rate=0.1, max_depth=3)\n",
    "# xgb2.fit(x_train, y_train)\n",
    "\n",
    "# xgb3 = XGBRegressor(booster='gblinear', n_estimators=1500, learning_rate=0.1, max_depth=3)\n",
    "# xgb3.fit(x_train, y_train)\n",
    "\n",
    "# xgb4 = XGBRegressor(booster='gblinear', n_estimators=700, learning_rate=0.1, max_depth=3)\n",
    "# xgb4.fit(x_train, y_train)\n",
    "# xgb5 = XGBRegressor(booster='gblinear', n_estimators=900, learning_rate=0.1, max_depth=3)\n",
    "# xgb5.fit(x_train, y_train)\n",
    "\n",
    "# print('GBDT:', mean_squared_error(y_validate, gbr.predict(x_validate)))\n",
    "# # print('Random Forests:', mean_squared_error(y_validate, rfr.predict(x_validate)))\n",
    "# print('LASSO:', mean_squared_error(y_validate, lsr.predict(x_validate)))\n",
    "# print('Ridge Regression: ', mean_squared_error(y_validate, rgr.predict(x_validate)))\n",
    "# print('XGBoost', mean_squared_error(y_validate, xgb.predict(x_validate)))\n",
    "# print('XGBoost', mean_squared_error(y_validate, xgb2.predict(x_validate)))\n",
    "# print('XGBoost', mean_squared_error(y_validate, xgb3.predict(x_validate)))\n",
    "\n",
    "# print(mean_squared_error(y_validate, (1*gbr.predict(x_validate) + 1*lsr.predict(x_validate) + 1*rgr.predict(x_validate) + xgb2.predict(x_validate)+ xgb3.predict(x_validate))/5))\n",
    "# print(mean_squared_error(y_validate, (1*gbr.predict(x_validate) + 1*lsr.predict(x_validate) + 1*rgr.predict(x_validate))/3))\n",
    "# print(mean_squared_error(y_validate, (1*xgb.predict(x_validate) + 1*xgb2.predict(x_validate) + 1*xgb3.predict(x_validate) + 1*xgb4.predict(x_validate) + 1*xgb5.predict(x_validate))/5))\n",
    "# print(mean_squared_error(y_validate, (rgr.predict(x_validate)+lsr.predict(x_validate))/2))\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 108757015119906370128420911575990272.00000000\n",
      "Iteration 2, loss = 24833896801988027468480512.00000000\n",
      "MLP: 4.2669045803667755e+27\n"
     ]
    }
   ],
   "source": [
    "# mlpr = MLPRegressor(hidden_layer_sizes=(4000, 4000, 4000, 4000, 4000, 4000, 4000, 4000),\n",
    "#                     alpha=0.0001,\n",
    "#                     learning_rate='adaptive',\n",
    "#                     learning_rate_init=0.01,\n",
    "#                     max_iter = 500,\n",
    "#                     verbose = True,\n",
    "#                     early_stopping = False,\n",
    "#                    )\n",
    "# mlpr.fit(x_train, y_train)\n",
    "# print('MLP:', mean_squared_error(y_validate, mlpr.predict(x_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbr = GradientBoostingRegressor(n_estimators=500, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n",
    "# gbr.fit(train_x, train_y)\n",
    "# rfr = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=1000)\n",
    "# rfr.fit(train_x, train_y)\n",
    "# lsr= Lasso(alpha=0.01, normalize=False)\n",
    "# lsr.fit(x_train, y_train)\n",
    "# rgr= Ridge(alpha=0.01)\n",
    "# rgr.fit(x_train, y_train)\n",
    "xgb = XGBRegressor(booster='gblinear', n_estimators=1100, learning_rate=0.1, max_depth=3)\n",
    "xgb.fit(x_train, y_train)\n",
    "xgb2 = XGBRegressor(booster='gblinear', n_estimators=1300, learning_rate=0.1, max_depth=2)\n",
    "xgb2.fit(x_train, y_train)\n",
    "xgb3 = XGBRegressor(booster='gblinear', n_estimators=1500, learning_rate=0.1, max_depth=1)\n",
    "xgb3.fit(x_train, y_train)\n",
    "xgb4 = XGBRegressor(booster='gblinear', n_estimators=700, learning_rate=0.1, max_depth=2)\n",
    "xgb4.fit(x_train, y_train)\n",
    "xgb5 = XGBRegressor(booster='gblinear', n_estimators=900, learning_rate=0.1, max_depth=3)\n",
    "xgb5.fit(x_train, y_train)\n",
    "\n",
    "# # predict = (1*gbr.predict(testset) + 1*lsr.predict(testset) + 1*rgr.predict(testset))/3\n",
    "# predict = gbr.predict(testset)\n",
    "# predicts = rgr.predict(test_x)\n",
    "predicts = ((1*xgb.predict(test_x) + 1*xgb2.predict(test_x) + 1*xgb3.predict(test_x) + 1*xgb4.predict(test_x) +1*xgb5.predict(test_x))/5)\n",
    "# predicts = (1*lsr.predict(test_x) + 1*rgr.predict(test_x))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_rgs = MLPRegressor(hidden_layer_sizes=(2000,2000,2000,2000), activation='relu')\n",
    "# nn_rgs.fit(x_train, y_train)\n",
    "# y_predict = nn_rgs.predict(x_validate)\n",
    "# score = mean_squared_error(y_validate, y_predict)\n",
    "\n",
    "# score = cross_val_score(nn_rgs, train_x, train_y, cv=8, scoring='mean_squared_error')\n",
    "# print(score)\n",
    "# np.mean(score)\n",
    "# predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_rgs.fit(train_x, train_y)\n",
    "# predict = nn_rgs.predict(testset)\n",
    "predicts = predicts.tolist()\n",
    "predicts = list(map(lambda x: x if x>0 else 0, predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['Id'] = testset.index\n",
    "result['time'] = pd.Series(p)\n",
    "result.to_csv('./submition_20527456_6st.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "scientific"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
